{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "\n",
    "- Download datasets from links below and run commented code in **Data** section, that will prepare folder with images.\n",
    "- Fold everything for better understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "Texts: [https://www.kaggle.com/datasets/naderabdalghani/iam-handwritten-forms-dataset](https://www.kaggle.com/datasets/naderabdalghani/iam-handwritten-forms-dataset)\n",
    "\n",
    "Words: [https://www.kaggle.com/datasets/nibinv23/iam-handwriting-word-database](https://www.kaggle.com/datasets/nibinv23/iam-handwriting-word-database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import tensor\n",
    "from torch.functional import F\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision import models\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from skimage.morphology import skeletonize\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import copy\n",
    "import cv2\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    MAX_LEN = 12\n",
    "    vocab = \" abcdefghijklmnopqrstuvwxyz\" # 1-indexed\n",
    "    char_to_index = {char: i for i, char in enumerate(vocab)}\n",
    "    index_to_char = {i: char for i, char in enumerate(vocab)}\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_string(s: str):\n",
    "        return [Params.char_to_index[char] for char in s]\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_string(encoded: list[int]):\n",
    "        return ''.join([Params.index_to_char[i] for i in encoded if i != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss: float, model: nn.Module):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_loader(loader: DataLoader, procent: float):\n",
    "    dataset = loader.dataset\n",
    "    indices = torch.randperm(len(dataset))[:int(len(dataset) * procent)]\n",
    "    dataset = torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset, batch_size=loader.batch_size, num_workers=4, shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get list of images and labels\n",
    "# with open('iam_words/words.txt', 'r') as f:\n",
    "#     words = f.readlines()\n",
    "# words = [word.strip() for word in words]\n",
    "# words = words[18:-1]\n",
    "# words = [w for w in words if ' err ' not in w]\n",
    "# words = [[w.split(' ')[0], w.split(' ')[-1]] for w in words]\n",
    "# words = [\n",
    "#     [f'iam_words/words/{w.split('-')[0]}/{w.split('-')[0]}-{w.split('-')[1]}/{w}.png', y] for w, y in words]\n",
    "# df = pd.DataFrame(words, columns=['filename', 'word'])\n",
    "# df = df[df['filename'].apply(os.path.exists)]\n",
    "\n",
    "\n",
    "\n",
    "# # Filter out invalid images\n",
    "# valid_rows = []\n",
    "# for i, (path, label) in df.iterrows():\n",
    "#     try:\n",
    "#         with Image.open(path) as img:\n",
    "#             img.verify()  # Verify that the file is a valid image\n",
    "#         valid_rows.append((path, label))\n",
    "#     except Exception as e:\n",
    "#         print(f\"Skipping file {path} due to error: {e}\")\n",
    "#     if i % (len(df) //100 + 1) == 0:\n",
    "#         print('.', end='')\n",
    "# print('\\n')\n",
    "\n",
    "# df = pd.DataFrame(valid_rows, columns=df.columns)\n",
    "# df = df[df['word'].apply(lambda x: all(char in Params.vocab[1:]\n",
    "#                          for char in x) and len(x) <= Params.MAX_LEN)]\n",
    "\n",
    "\n",
    "\n",
    "# # Split data into sets convinient for torch dataset\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "# train_df = train_df.reset_index(drop=True)\n",
    "# val_df = val_df.reset_index(drop=True)\n",
    "# test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# # Save images in convinient for torch way\n",
    "# shutil.rmtree('words_data', ignore_errors=True)\n",
    "\n",
    "# for i, (im, label) in train_df.iterrows():\n",
    "#     new_path = f'words_data/train/{label.ljust(Params.MAX_LEN, \"_\")}'\n",
    "#     if not os.path.exists(new_path):\n",
    "#         os.makedirs(new_path)\n",
    "#     im = Image.open(im)\n",
    "#     im.save(f'{new_path}/{i}.png')\n",
    "#     if i % (len(train_df) // 100 + 1) == 0:\n",
    "#         print('.', end='')\n",
    "# print()\n",
    "\n",
    "# for i, (im, label) in val_df.iterrows():\n",
    "#     new_path = f'words_data/val/{label.ljust(Params.MAX_LEN, \"_\")}'\n",
    "#     if not os.path.exists(new_path):\n",
    "#         os.makedirs(new_path)\n",
    "#     im = Image.open(im)\n",
    "#     im.save(f'{new_path}/{i}.png')\n",
    "#     if i % (len(val_df) // 100 + 1) == 0:\n",
    "#         print('.', end='')\n",
    "# print()\n",
    "\n",
    "\n",
    "# for i, (im, label) in test_df.iterrows():\n",
    "#     new_path = f'words_data/test/{label.ljust(Params.MAX_LEN, \"_\")}'\n",
    "#     if not os.path.exists(new_path):\n",
    "#         os.makedirs(new_path)\n",
    "#     im = Image.open(im)\n",
    "#     im.save(f'{new_path}/{i}.png')\n",
    "#     if i % (len(test_df) // 100 + 1) == 0:\n",
    "#         print('.', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandWritingDataset(datasets.ImageFolder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        transform: transforms.Compose = None,\n",
    "        label_transofrm=None,\n",
    "        augument: transforms.Compose = None\n",
    "    ):\n",
    "        if augument is not None:\n",
    "            transform = transforms.Compose([\n",
    "                augument,\n",
    "                transform\n",
    "            ])\n",
    "\n",
    "        super(HandWritingDataset, self).__init__(root, transform=transform)\n",
    "        self.classes = [''.join([i if i != '_' else ' ' for i in word]) for word in self.classes]\n",
    "        self.label_transform = label_transofrm\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = super(HandWritingDataset, self).__getitem__(index)\n",
    "        if self.label_transform is not None:\n",
    "            label = tensor(self.label_transform(self.classes[label]))\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "augument = transforms.Compose([\n",
    "    transforms.RandomRotation(15, expand=True, fill=(255,)),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1), fill=(255,)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5, fill=(255,)),\n",
    "    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: 1.0 - x),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HandWritingDataset(\n",
    "    root='words_data/train',\n",
    "    transform=transform,\n",
    "    label_transofrm=Params.encode_string,\n",
    "    augument=augument\n",
    ")\n",
    "\n",
    "val_dataset = HandWritingDataset(\n",
    "    root='words_data/val',\n",
    "    transform=transform,\n",
    "    label_transofrm=Params.encode_string\n",
    ")\n",
    "\n",
    "test_dataset = HandWritingDataset(\n",
    "    root='words_data/test',\n",
    "    transform=transform,\n",
    "    label_transofrm=Params.encode_string\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count weights of word frequency for potential ballancing dataset\n",
    "class_counts = np.bincount([label for _, label in train_dataset.samples])\n",
    "class_weights = 1.0 / class_counts\n",
    "sample_weights = [class_weights[label] for _, label in train_dataset.samples]\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    # sampler=sampler\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: nn.Module, dataloader: DataLoader, criterion: nn.Module):\n",
    "    model.eval()\n",
    "    all_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            batch_size = images.size(0)\n",
    "            input_lengths = torch.full(\n",
    "                size=(batch_size,), fill_value=outputs.size(0), dtype=torch.long).to(device)\n",
    "            target_lengths = torch.tensor(\n",
    "                [len(seq[seq != 0]) for seq in labels], dtype=torch.long).to(device)\n",
    "\n",
    "            all_loss += criterion(outputs, labels, input_lengths, target_lengths)\n",
    "\n",
    "    loss = all_loss / len(dataloader)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, optimizer: optim.Optimizer, criterion: nn.Module, epochs: int, early_stopping: EarlyStopping = None):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        progress_bar = -1\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            input_lengths = torch.full(\n",
    "                size=(batch_size,), fill_value=outputs.size(0), dtype=torch.long).to(device)\n",
    "            target_lengths = torch.tensor(\n",
    "                [len(seq[seq != 0]) for seq in labels], dtype=torch.long).to(device)\n",
    "            \n",
    "            loss = criterion(outputs, labels, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Print progress bar\n",
    "            new_progress_bar = i * 100 // len(train_loader)\n",
    "            if new_progress_bar > progress_bar:\n",
    "                for j in range(new_progress_bar - progress_bar):\n",
    "                    print('=', end='', flush=True)\n",
    "                progress_bar = new_progress_bar\n",
    "\n",
    "        # Print epoch summary\n",
    "        val_loss = test(model, val_loader, criterion)\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}, Train Loss: {100 * train_loss:.4f}, Val Loss: {100 * val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping and restore best model\n",
    "        if early_stopping:\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                model.load_state_dict(early_stopping.best_model)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: nn.Module, dataloader: DataLoader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).squeeze(0)\n",
    "            labels = labels.squeeze(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend([Params.decode_string(preds.cpu().numpy())])\n",
    "            all_labels.extend([Params.decode_string(labels.cpu().numpy())])\n",
    "    return list(zip(all_preds, all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLModel(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(TLModel, self).__init__()\n",
    "        self.mnv2 = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V2).features[:-1]\n",
    "        self.set_trainable(False)\n",
    "        self.lstm = nn.LSTM(input_size=5120, hidden_size=256, num_layers=2,\n",
    "                            batch_first=True, bidirectional=True, dropout=0.5)\n",
    "        self.dense = nn.Linear(512, 27)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mnv2(x)\n",
    "\n",
    "        x = x.permute(0, 3, 2, 1)\n",
    "        x = x.flatten(2)\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = self.dense(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = F.log_softmax(x, dim=2)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def set_trainable(self, trainable: bool):\n",
    "        for param in self.mnv2.parameters():\n",
    "            param.requires_grad = trainable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TLModel().to(device)\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "to_train_loader = sample_loader(train_loader, 0.02)\n",
    "# to_train_loader = train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================\n",
      "Epoch 1/1, Train Loss: 8.0592, Val Loss: 8.3368\n"
     ]
    }
   ],
   "source": [
    "train(model, to_train_loader, val_loader, optimizer, criterion, epochs=1, early_stopping=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_trainable(True)\n",
    "early_stopping = EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================\n",
      "Epoch 1/1, Train Loss: 7.9183, Val Loss: 8.2385\n"
     ]
    }
   ],
   "source": [
    "train(model, to_train_loader, val_loader, optimizer, criterion, epochs=1, early_stopping=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "torch.save(model, f\"models/model_{timestamp}.pth\")\n",
    "model = torch.load(f\"models/model_{timestamp}.pth\", weights_only=False)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('v', 'by'),\n",
       " ('g', 'with'),\n",
       " ('o', 'a'),\n",
       " ('s', 'tried'),\n",
       " ('o', 'years'),\n",
       " ('o', 'in'),\n",
       " ('o', 'change'),\n",
       " ('a', 'on'),\n",
       " ('k', 'general'),\n",
       " ('o', 'are'),\n",
       " ('k', 'attainable'),\n",
       " ('r', 'big'),\n",
       " ('a', 'cheap'),\n",
       " ('t', 'all'),\n",
       " ('n', 'at'),\n",
       " ('v', 'whether'),\n",
       " ('n', 'wet'),\n",
       " ('l', 'in'),\n",
       " ('a', 'is'),\n",
       " ('a', 'margin'),\n",
       " ('o', 'most'),\n",
       " ('o', 'for'),\n",
       " ('a', 'was'),\n",
       " ('k', 'returned'),\n",
       " ('v', 'advantage'),\n",
       " ('o', 'for'),\n",
       " ('o', 'a'),\n",
       " ('f', 'made')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_loader = sample_loader(test_loader, 0.01)\n",
    "\n",
    "predict(model, show_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
