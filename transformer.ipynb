{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "3f39f763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import tensor\n",
    "from torch.functional import F\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision import models\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from skimage.morphology import skeletonize\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import copy\n",
    "import cv2\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import h5py\n",
    "from Helpers import Params, collate_fn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "4586b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressBar:\n",
    "    def __init__(self, total: int):\n",
    "        self.total = total\n",
    "        self.current = 0\n",
    "        self.last_progress = 0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def update(self, current: int, epochs: str):\n",
    "        self.current = current\n",
    "        progress = (self.current / self.total) * 100\n",
    "        if int(progress) > self.last_progress:\n",
    "            elapsed_time = time.time() - self.start_time\n",
    "            print(\n",
    "                f'\\rEpoch: {epochs.rjust(7)} {str(int(progress)).rjust(3)}% | Elapsed: {str(int(elapsed_time)).rjust(3)}s', end='')\n",
    "            self.last_progress = int(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "f4383336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, path='checkpoint.pt', verbose=False):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "2a4c675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHistory:\n",
    "    def __init__(self):\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "\n",
    "    def update(self, train_loss: float, train_accuracy: float, val_loss: float, val_accuracy: float):\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        self.history['train_accuracy'].append(train_accuracy)\n",
    "        self.history['val_loss'].append(val_loss)\n",
    "        self.history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(1, len(self.history['train_loss']) + 1)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(epochs, self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(\n",
    "            epochs, self.history['train_accuracy'], label='Train Accuracy')\n",
    "        plt.plot(epochs, self.history['val_accuracy'],\n",
    "                 label='Validation Accuracy')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "0abfad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHistory:\n",
    "    def __init__(self):\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "\n",
    "    def update(self, train_loss: float, train_accuracy: float, val_loss: float, val_accuracy: float):\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        self.history['train_accuracy'].append(train_accuracy)\n",
    "        self.history['val_loss'].append(val_loss)\n",
    "        self.history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(1, len(self.history['train_loss']) + 1)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(epochs, self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(\n",
    "            epochs, self.history['train_accuracy'], label='Train Accuracy')\n",
    "        plt.plot(epochs, self.history['val_accuracy'],\n",
    "                 label='Validation Accuracy')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "37d887b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def update(self, loss: float, accuracy: float):\n",
    "        self.losses.append(loss)\n",
    "        self.accuracies.append(accuracy)\n",
    "\n",
    "    def get_average_loss(self):\n",
    "        return sum(self.losses) / len(self.losses) if self.losses else 0.0\n",
    "\n",
    "    def get_average_accuracy(self):\n",
    "        return sum(self.accuracies) / len(self.accuracies) if self.accuracies else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "848ed791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator:\n",
    "    def __init__(self, model: nn.Module, criterion: nn.CrossEntropyLoss, device: torch.device):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "    def validate(self, val_loader: DataLoader) -> tuple:\n",
    "        self.model.eval()\n",
    "        metrics_tracker = MetricsTracker()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image, midi in val_loader:\n",
    "                image: torch.Tensor = image.to(self.device)\n",
    "                midi: torch.Tensor = midi.to(self.device)\n",
    "\n",
    "                input_tokens: torch.Tensor = midi[:, :-1]\n",
    "                target_tokens: torch.Tensor = midi[:, 1:]\n",
    "                target_tokens = target_tokens.reshape(-1)\n",
    "\n",
    "                output: torch.Tensor = self.model(image, input_tokens)\n",
    "                output = output.reshape(-1, output.shape[-1])\n",
    "\n",
    "                loss: torch.Tensor = self.criterion(output, target_tokens)\n",
    "                accuracy = (output.argmax(dim=1) ==\n",
    "                            target_tokens).float().mean().item()\n",
    "\n",
    "                metrics_tracker.update(loss.item(), accuracy)\n",
    "\n",
    "        avg_loss = metrics_tracker.get_average_loss()\n",
    "        avg_accuracy = metrics_tracker.get_average_accuracy()\n",
    "\n",
    "        return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "099553aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "3eb56fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidatorWithOutputs:\n",
    "    def __init__(self, model: nn.Module, criterion: nn.CrossEntropyLoss, device: torch.device):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "    def validate(self, val_loader: DataLoader) -> tuple:\n",
    "        self.model.eval()\n",
    "        outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image, midi in val_loader:\n",
    "                image: torch.Tensor = image.to(self.device)\n",
    "                midi: torch.Tensor = midi.to(self.device)\n",
    "\n",
    "                input_tokens: torch.Tensor = midi[:, :-1]\n",
    "                target_tokens: torch.Tensor = midi[:, 1:]\n",
    "                target_tokens = target_tokens.reshape(-1)\n",
    "\n",
    "                output: torch.Tensor = self.model(image, input_tokens)\n",
    "                output = output.reshape(-1, output.shape[-1])\n",
    "\n",
    "                predicted = output.argmax(dim=1).cpu().tolist()\n",
    "                expected = target_tokens.cpu().tolist()\n",
    "\n",
    "                outputs.append((predicted, expected))\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "7ac0ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epochs: int,\n",
    "    device: torch.device = device,\n",
    "    history: TrainingHistory = None,\n",
    "    early_stopping: EarlyStopping = None\n",
    "):\n",
    "    for epoch in range(epochs):\n",
    "        progress_bar = ProgressBar(len(train_loader))\n",
    "        metrics_tracker = MetricsTracker()\n",
    "        validator = Validator(model, criterion, device)\n",
    "        model.train()\n",
    "\n",
    "        for i, (image, word) in enumerate(train_loader):\n",
    "            image: torch.Tensor = image.to(device)\n",
    "            word: torch.Tensor = word.to(device)\n",
    "\n",
    "            input_tokens = word[:, :-1]\n",
    "            target_tokens = word[:, 1:]\n",
    "            # print(input_tokens)\n",
    "            # print(target_tokens)\n",
    "            target_tokens = target_tokens.reshape(-1)\n",
    "\n",
    "            output: torch.Tensor = model(image, input_tokens)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "\n",
    "            loss: torch.Tensor = criterion(output, target_tokens)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracy = (output.argmax(dim=1) == target_tokens).float().mean().item()\n",
    "            metrics_tracker.update(loss.item(), accuracy)\n",
    "\n",
    "            progress_bar.update(i + 1, f'{epoch + 1}/{epochs}')\n",
    "\n",
    "        print(\n",
    "            f' | loss: {metrics_tracker.get_average_loss():.4f} - acc: {metrics_tracker.get_average_accuracy():.4f}', end='')\n",
    "        val_loss, val_acc = validator.validate(val_loader)\n",
    "        print(f' | val_loss: {val_loss:.4f} - val_acc: {val_acc:.4f}')\n",
    "\n",
    "        if history:\n",
    "            history.update(metrics_tracker.get_average_loss(\n",
    "            ), metrics_tracker.get_average_accuracy(), val_loss, val_acc)\n",
    "\n",
    "        if early_stopping:\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "97052507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandWritingDataset(datasets.ImageFolder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        transform: transforms.Compose = None,\n",
    "        label_transofrm=None,\n",
    "        augument: transforms.Compose = None\n",
    "    ):\n",
    "        if augument is not None:\n",
    "            transform = transforms.Compose([\n",
    "                augument,\n",
    "                transform\n",
    "            ])\n",
    "\n",
    "        super(HandWritingDataset, self).__init__(root, transform=transform)\n",
    "        self.classes = [''.join([i if i != '_' else '' for i in word])\n",
    "                        for word in self.classes]\n",
    "        self.label_transform = label_transofrm\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = super(HandWritingDataset, self).__getitem__(index)\n",
    "        if self.label_transform is not None:\n",
    "            label = tensor(self.label_transform(self.classes[label]))\n",
    "        label = torch.cat((tensor([27]), label, tensor([28])))\n",
    "        return image, label\n",
    "\n",
    "    def create_sampler(self, epsilon: float = 0.004):\n",
    "        \"\"\"\n",
    "        epsilon: how strongly flatten the distribution? 0 means flat, the greater the more similar to original distribution, after 0.01 there is little difference\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self.samples, columns=['filename', 'class'])\n",
    "        df['label'] = df['class'].apply(lambda x: ''.join(\n",
    "            [i for i in self.classes[x] if i != ' ']))\n",
    "        df['length'] = df['label'].apply(len)\n",
    "        length_df = df['length'].value_counts().sort_index().to_dict()\n",
    "        class_count = df['class'].value_counts().sort_index()\n",
    "        df['class_length'] = df['length'].map(length_df)\n",
    "        df['class_count'] = df['class'].apply(lambda x: class_count.iloc[x])\n",
    "        df['result'] = 1.0 / df['class_length'] + epsilon\n",
    "        return WeightedRandomSampler(df['result'].values, len(df['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "cd79c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5Dataset:\n",
    "    def __init__(self, file_path: str, num_epochs: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_epochs (int): Number of epochs to split the dataset into.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        with h5py.File(file_path, 'r') as h5f:\n",
    "            h5_size = len(h5f['images'])\n",
    "            self.num_epochs = num_epochs\n",
    "            self.epoch_size = h5_size // num_epochs\n",
    "            self.current_epoch = 0\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epoch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with h5py.File(self.file_path, 'r') as h5f:\n",
    "            image = h5f['images'][self.current_epoch * self.epoch_size + index]\n",
    "            image = self.transform(image)\n",
    "            image = image.permute(1, 2, 0)\n",
    "            image = torch.stack([image[0]] * 3, dim=0)\n",
    "\n",
    "            label = h5f['labels'][self.current_epoch * self.epoch_size + index]\n",
    "            label = label[:np.argmax(label == 0)] if 0 in label else label\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            label = torch.cat((tensor([27]), label, tensor([28])))\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def next_epoch(self):\n",
    "        self.current_epoch = (self.current_epoch + 1) % self.num_epochs\n",
    "\n",
    "    def create_h5_sampler(self, epsilon: float = 0.004):\n",
    "        \"\"\"\n",
    "        epsilon: how strongly flatten the distribution? 0 means flat, the greater the more similar to original distribution, after 0.01 there is little difference\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame({'index': range(\n",
    "            self.current_epoch * self.epoch_size, (self.current_epoch + 1) * self.epoch_size)})\n",
    "        with h5py.File(self.file_path, 'r') as h5f:\n",
    "            labels = h5f['labels'][self.current_epoch *\n",
    "                                   self.epoch_size: (self.current_epoch + 1) * self.epoch_size]\n",
    "            df['length'] = (labels != 0).sum(axis=1)\n",
    "\n",
    "        length_df = df['length'].value_counts().sort_index()\n",
    "        length_dict = length_df.to_dict()\n",
    "\n",
    "        df['class_length'] = df['length'].map(length_dict)\n",
    "        df['result'] = 1.0 / df['class_length'] + epsilon\n",
    "\n",
    "        return WeightedRandomSampler(df['result'].values, len(df['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "43bc23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 128,\n",
    "        nhead_en: int = 1,\n",
    "        num_layers_en: int = 1,\n",
    "        nhead_de: int = 1,\n",
    "        num_layers_de: int = 1,\n",
    "        dropout: float = 0.2  # Added dropout parameter\n",
    "    ):\n",
    "        super(HandwritingTransformer, self).__init__()\n",
    "\n",
    "        self.cnn = models.mobilenet_v2(pretrained=True).features[:4]\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.input_size = nn.Linear(input_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, 200)\n",
    "\n",
    "        # Add LayerNorm and Dropout after input_size\n",
    "        self.input_norm = nn.LayerNorm(d_model)\n",
    "        self.input_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead_en, dropout=dropout, norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers_en)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model, nhead_de, dropout=dropout, norm_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers_de)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, 20)\n",
    "\n",
    "        # Add LayerNorm and Dropout after embedding\n",
    "        self.embedding_norm = nn.LayerNorm(d_model)\n",
    "        self.embedding_dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
    "        src = torch.stack([src[:, 0]] * 3, dim=1)\n",
    "        src = self.cnn(src)\n",
    "\n",
    "        src = src.flatten(1, 2)\n",
    "        src = src.permute(0, 2, 1)\n",
    "        src = self.input_size(src)\n",
    "\n",
    "        # Apply LayerNorm and Dropout after input_size\n",
    "        src = self.input_norm(src)\n",
    "        src = self.input_dropout(src)\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        memory = self.encoder(src)\n",
    "\n",
    "        tgt = self.embedding(tgt)\n",
    "\n",
    "        # Apply LayerNorm and Dropout after embedding\n",
    "        tgt = self.embedding_norm(tgt)\n",
    "        tgt = self.embedding_dropout(tgt)\n",
    "\n",
    "        tgt = self.pos_decoder(tgt)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(\n",
    "            tgt.size(0)).to(tgt.device)\n",
    "\n",
    "        output: torch.Tensor = self.decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        output = self.output(output)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        return output\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "138cdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "augument = transforms.Compose([\n",
    "    # transforms.RandomRotation(15, expand=True, fill=(255,)),\n",
    "    transforms.RandomAffine(0, translate=(0.05, 0.05), fill=(255,)),\n",
    "    # transforms.RandomPerspective(distortion_scale=0.5, p=0.5, fill=(255,)),\n",
    "    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((64, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: 1.0 - x),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "7f6580b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/miniconda3/envs/torch/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jakub/miniconda3/envs/torch/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/jakub/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = HandWritingDataset('words_data/train', transform=transform, label_transofrm=Params.encode_string, augument=augument) # augument data\n",
    "train_dataset = H5Dataset('train_data.h5', num_epochs=1)\n",
    "val_dataset = HandWritingDataset(root='words_data/val', transform=transform, label_transofrm=Params.encode_string)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=4, sampler=train_dataset.create_h5_sampler(0), collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, num_workers=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = HandwritingTransformer(\n",
    "    input_size=16 * 24,\n",
    "    vocab_size=len(Params.vocab)+2,\n",
    "    d_model=128,\n",
    "    nhead_en=1,\n",
    "    num_layers_en=1,\n",
    "    nhead_de=1,\n",
    "    num_layers_de=1,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "history = TrainingHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "d6233e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/50 100% | Elapsed: 237s | loss: 1.3753 - acc: 0.3237 | val_loss: 0.8159 - val_acc: 0.3374\n",
      "Epoch:    2/50 100% | Elapsed: 266s | loss: 0.9848 - acc: 0.3938 | val_loss: 0.6351 - val_acc: 0.3631\n",
      "Epoch:    3/50 100% | Elapsed: 273s | loss: 0.8736 - acc: 0.4147 | val_loss: 0.6051 - val_acc: 0.3753\n",
      "Epoch:    4/50   1% | Elapsed:   3s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[318]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[311]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, epochs, device, history, early_stopping)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# print(input_tokens)\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# print(target_tokens)\u001b[39;00m\n\u001b[32m     26\u001b[39m target_tokens = target_tokens.reshape(-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m output: torch.Tensor = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m output = output.reshape(-\u001b[32m1\u001b[39m, output.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m     31\u001b[39m loss: torch.Tensor = criterion(output, target_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[314]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mHandwritingTransformer.forward\u001b[39m\u001b[34m(self, src, tgt)\u001b[39m\n\u001b[32m     62\u001b[39m tgt = \u001b[38;5;28mself\u001b[39m.embedding_norm(tgt)\n\u001b[32m     63\u001b[39m tgt = \u001b[38;5;28mself\u001b[39m.embedding_dropout(tgt)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m tgt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m tgt = tgt.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     68\u001b[39m tgt_mask = \u001b[38;5;28mself\u001b[39m.generate_square_subsequent_mask(\n\u001b[32m     69\u001b[39m     tgt.size(\u001b[32m0\u001b[39m)).to(tgt.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    epochs=50,\n",
    "    early_stopping=early_stopping,\n",
    "    history=history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "5d6f4c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wven          -  even        \n",
      "tir           -  her         \n",
      "hese          -  have        \n",
      "oe            -  be          \n",
      "it            -  if          \n",
      "the           -  the         \n",
      "staongl       -  straight    \n",
      "of            -  of          \n",
      "aitk          -  sick        \n",
      "ot            -  at          \n",
      "especiably    -  especially  \n",
      "to            -  to          \n",
      "through       -  through     \n",
      "on            -  on          \n",
      "molleon       -  million     \n",
      "unioorm       -  uniform     \n",
      "as            -  is          \n",
      "enay          -  away        \n",
      "oboolutely    -  absolutely  \n",
      "eggur         -  enjoy       \n",
      "up            -  up          \n",
      "a             -  a           \n",
      "to            -  to          \n",
      "oll           -  all         \n",
      "hheught       -  though      \n",
      "fre           -  free        \n",
      "is            -  in          \n",
      "of            -  of          \n",
      "enderpinning  -  underpinning\n",
      "wodds         -  moods       \n",
      "planwd        -  played      \n",
      "tor           -  for         \n",
      "one           -  one         \n",
      "growing       -  growing     \n",
      "juitefied     -  justified   \n",
      "mmtoherently  -  incoherently\n",
      "teveev        -  review      \n",
      "secord        -  second      \n",
      "the           -  the         \n",
      "sefure        -  refuse      \n",
      "oane          -  case        \n",
      "that          -  that        \n",
      "wnfomr        -  unfair      \n",
      "of            -  of          \n",
      "a             -  a           \n",
      "is            -  is          \n",
      "peace         -  peace       \n",
      "incophrity    -  incapacity  \n",
      "tor           -  for         \n",
      "it            -  it          \n",
      "of            -  of          \n",
      "nisd          -  mind        \n",
      "will          -  will        \n",
      "go            -  go          \n",
      "sight         -  sight       \n",
      "a             -  a           \n",
      "wite          -  wife        \n",
      "effective     -  effective   \n",
      "heid          -  said        \n",
      "modusate      -  modulate    \n",
      "weady         -  ready       \n",
      "ot            -  at          \n",
      "industry      -  industry    \n",
      "tast          -  task        \n",
      "his           -  has         \n",
      "dupgest       -  suggest     \n",
      "aeatlt        -  desert      \n",
      "and           -  and         \n",
      "of            -  of          \n",
      "sas           -  was         \n",
      "of            -  of          \n",
      "would         -  would       \n",
      "film          -  film        \n",
      "in            -  in          \n",
      "this          -  this        \n",
      "is            -  is          \n",
      "of            -  of          \n",
      "into          -  into        \n",
      "that          -  that        \n",
      "with          -  with        \n",
      "baiiien       -  barrier     \n",
      "te            -  he          \n",
      "sidee         -  silver      \n",
      "tutt          -  best        \n",
      "they          -  they        \n",
      "and           -  and         \n",
      "but           -  but         \n",
      "to            -  to          \n",
      "is            -  in          \n",
      "promg         -  proud       \n",
      "the           -  the         \n",
      "likeer        -  listen      \n",
      "meault        -  result      \n",
      "in            -  in          \n",
      "they          -  they        \n",
      "bimittd       -  limited     \n",
      "a             -  a           \n",
      "doer          -  does        \n",
      "ouder         -  order       \n",
      "is            -  is          \n",
      "who           -  who         \n"
     ]
    }
   ],
   "source": [
    "test_dataset = HandWritingDataset(root='words_data/val', transform=transform, label_transofrm=Params.encode_string)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_with_outs = ValidatorWithOutputs(model, criterion, device)\n",
    "outputs = val_with_outs.validate(test_loader)\n",
    "\n",
    "for i, (predicted, expected) in enumerate(outputs):\n",
    "    if i > 100:\n",
    "        break\n",
    "    predicted = Params.decode_string([i for i in predicted if i not in [27, 28]])\n",
    "    expected = Params.decode_string([i for i in expected if i not in [27, 28]])\n",
    "    print(f\"{predicted.ljust(12)}  -  {expected.ljust(12)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
